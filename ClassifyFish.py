# -*- coding: utf-8 -*-
"""demo01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qlX1uxTmPSXI8BcHke_IT1SWmvs17zPE
"""

#kết nối với các thư viện của keras, sklearn, numpy, matplotlib, os và cv2
from numpy import loadtxt
from sklearn.model_selection import train_test_split
from keras.models import load_model
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten, Dropout, BatchNormalization
import numpy as np
from IPython.display import display, Image
import os
import cv2
from keras.utils import to_categorical
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder

#kết nối đến drive
from google.colab import drive
drive.mount('/content/drive')
drive_path = '/content/drive/My Drive/Colab Notebooks/'
directory_path1  = drive_path + 'images/cropped'
directory_path2  = drive_path + 'images/test_img'

cnt = {}
for filename in os.listdir(directory_path1):
    if os.path.isfile(os.path.join(directory_path1, filename)):
        name = filename.split("_")[0]
        cnt[name] = cnt.get(name, 0) + 1

#chia img và label
sorted_dict = dict(sorted(cnt.items(), key=lambda item: item[1], reverse=True))
top10dict = dict(list(sorted_dict.items())[:45])
outtop10dict = dict(list(sorted_dict.items())[45:90])
X = []
y = []
for directory_path in [directory_path1]:
  for filename in os.listdir(directory_path):
    if os.path.isfile(os.path.join(directory_path, filename)):
        name = filename.split("_")[0]
        if name in top10dict:
          X.append(cv2.imread(os.path.join(directory_path, filename)))
          y.append(name)
        elif name in outtop10dict:
          X.append(cv2.imread(os.path.join(directory_path, filename)))
          y.append('11')

#tạo tập train,test và val với tỉ lệ 4-3-3
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Kiểm tra xem có ảnh nào trống không
for img in X_train:
    if img is None or img.size == 0:
        print("Có ảnh trống")

#Xóa ảnh trốn
X_train = [img for img in X_train if img is not None and img.size != 0]
X_test= [img for img in X_test if img is not None and img.size != 0]
X_val= [img for img in X_val if img is not None and img.size != 0]

#resize về kích thước 244*244
X_train = [cv2.resize(img, (224, 224)) for img in X_train]
X_val = [cv2.resize(img, (224, 224)) for img in X_val]
X_test = [cv2.resize(img, (224, 224)) for img in X_test]

#chuẩn hóa pixal
X_train = np.array(X_train)
X_val = np.array(X_val)
X_test = np.array(X_test)

#train có 1332, test có 445, val có 444
X_train.shape
X_test.shape
X_val.shape

from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical

# Sử dụng LabelEncoder để chuyển đổi nhãn thành số nguyên
label_encoder = LabelEncoder()
y_train_encoded = label_encoder.fit_transform(y_train)
y_test_encoded = label_encoder.transform(y_test)
y_val_encoded = label_encoder.transform(y_val)
# Chuyển đổi thành dạng 1-hot
y_train_one_hot = to_categorical(y_train_encoded)
y_test_one_hot = to_categorical(y_test_encoded)
y_val_one_hot = to_categorical(y_val_encoded)

#tạo callback

from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger
#call back gồm
#lưu mô hình tốt nhất
checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)
#giảm learning rate khi mất mát không giảm
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)
# dừng đào tạo sớm nếu hàm mất mát ko cải thiện
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
#lưu log
csv_logger_path = os.path.join(drive_path, 'training.txt')
csv_logger = CSVLogger(csv_logger_path)

#tạo model với 5 lớp xử lý chính
#model của hình ảnh
model = Sequential()
#lớp chính thứ 1 gồm conv2d, batchNormalization, maxpooling và dropout với input là hình 224*224
model.add(Conv2D(256, kernel_size=(5, 5), activation='relu', input_shape=(224, 224, 3), padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(4, 4), strides=2))
model.add(Dropout(0.3))

model.add(Conv2D(128, kernel_size=(4, 4), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(3, 3), strides=2))
model.add(Dropout(0.3))

model.add(Conv2D(64, kernel_size=(4, 4), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(3, 3), strides=2))
model.add(Dropout(0.3))

model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
model.add(Dropout(0.3))

model.add(Conv2D(16, kernel_size=(2, 2), activation='relu', padding='same'))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2), strides=2))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(units=46, activation='softmax'))

#lập loss optimize metrics và kiểm tra cấu trúc
from tensorflow.keras.optimizers import Adam
optimizer = Adam(learning_rate=0.01)
model.compile(optimizer=optimizer,loss = 'categorical_crossentropy', metrics=['accuracy'])
model.summary()

#model text
modelText=Sequential()
modelText.add(Dense(44,input_dim=8,activation='relu' ))
modelText.add(Dense(22,activation='relu'))
modelText.add(Dense(11,activation='sigmoid'))
modelText.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])

#fit model

model.fit(X_train, y_train_one_hot, epochs=100, batch_size=32,validation_data=(X_val, y_val_one_hot),callbacks=[checkpoint, reduce_lr, early_stop,csv_logger])

#đánh giá loss và acc trên tập validation
loss,acc = model.evaluate(X_val, y_val_one_hot)
print("loss = ",loss)
print("acc = ",acc)

y_hat = model.predict(X_test[0:1])
y_hat

y_label=np.argmax(y_hat,axis=1)
y_label

plt.imshow(X_test[0])
print(y_test_one_hot[0])

#lưu model
 model.save("/content/drive/My Drive/Colab Notebooks/mymodel.h5")

